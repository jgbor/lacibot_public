{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai==0.28.1 --quiet \n",
    "%pip install langchain==0.0.320 --quiet\n",
    "%pip install chromadb==0.4.14 --quiet\n",
    "%pip install tiktoken==0.5.1--quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key beállítása\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraméterek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# használt erőforrások mappája\n",
    "res_folder = \"res/in_use/\"\n",
    "    \n",
    "# splitting paraméterek\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "# vector store paraméterek\n",
    "persist_directory = \"res/chroma/\"\n",
    "search_type = \"mmr\"\n",
    "search_k = 5\n",
    "search_fetch_k = 8\n",
    "lambda_mult = 0.6\n",
    "\n",
    "# memória\n",
    "memory_k = 3\n",
    "\n",
    "# ChatGPT paraméterek\n",
    "temperature = 0.4\n",
    "max_tokens = 500\n",
    "model_id = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erőforrásfájlok betöltése\n",
    "\n",
    "Ha már egyszer megtettük és van mentett vektor adatbázis, akkor nem kell újra futtatni.\n",
    "### CSV fájlok (nagyrészt kérdések) betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader, DirectoryLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(res_folder, glob=\"*.csv\", use_multithreading=True, loader_cls=CSVLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "csv_data = directory_loader.load()\n",
    "print(len(csv_data))\n",
    "#csv_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szöveges fájlok betöltése, majd feldarabolása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(res_folder, glob=\"*.txt\", use_multithreading=True, loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "text_data = directory_loader.load()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    "    separators=[\"\\n\\s*\\n\", \"\\n\\s*\", \"\\n\"]\n",
    ")\n",
    "\n",
    "split_text_data = text_splitter.split_documents(text_data)\n",
    "print(len(split_text_data))\n",
    "#split_text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStore inicializálása Chroma-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "# Embedding betöltése\n",
    "oai_embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elég egyszer futtatni, ha nem változtatunk az adatokon, mert lementi a vektoradatbázist.\n",
    "# Változás esetén törölni kell a res/chroma maappát, majd újra kell futtatni.\n",
    "combined_data = []\n",
    "combined_data.extend(split_text_data)\n",
    "combined_data.extend(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_vectordb = Chroma.from_documents(\n",
    "    documents=combined_data,\n",
    "    embedding=oai_embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "oai_vectordb.persist()\n",
    "vectordb = oai_vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha le van már mentve, ezt kell használni az előző cella helyett:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_vectordb = Chroma(persist_directory=persist_directory, embedding_function=oai_embedding)\n",
    "vectordb = oai_vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR teszt\n",
    "question = \"Mennyit kell dolgozni?\"\n",
    "search_result = vectordb.max_marginal_relevance_search(question,k = search_k, fetch_k = search_fetch_k, lambda_mult = lambda_mult)\n",
    "print(search_result)\n",
    "print(vectordb.similarity_search_with_relevance_scores(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memória config a chat historyhoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = memory_k, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot OpenAI-jal\n",
    "\n",
    "### Alap ConversationalRetrievalChain használatával"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model_name= model_id, temperature = temperature, max_tokens = max_tokens)\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever = vectordb.as_retriever(\n",
    "        searh_type = search_type,\n",
    "        search_kwargs = {\n",
    "            \"k\": search_k,\n",
    "            \"fetch_k\": search_fetch_k,\n",
    "            \"lambda_mult\": lambda_mult\n",
    "        }\n",
    "    ),\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saját prompttal\n",
    "\n",
    "Az előző cella helyett lehet ezt használni, ha nem a beépített dolgot szeretnénk használni, jobb eredményekért:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name= model_id, temperature = temperature, max_tokens = max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Előzmények alappján konkrét, önálló kérdést generálunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"A chat előzményekből és a következő inputból alakíts ki egy önmagában is helytálló kérdést, ha az input értelmezéséhez fontos kontextust tartalmaz!\n",
    "Ha az input nem kérdés vagy nem kapcsolódik már az előzményekhez, akkor csak add vissza az inputot!\n",
    "\n",
    "Chat előzmények:\n",
    "{chat_history}\n",
    "\n",
    "Következő input: {question}\n",
    "\n",
    "Átalakított kérdés:\"\"\"\n",
    "question_generator_prompt = PromptTemplate.from_template(template)\n",
    "question_generator_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=question_generator_prompt,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kontextus alapján válasz generálása:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = \"\"\"A BME VIK szakmai gyakorlat kérdéseire válaszoló chatbot vagy. A kérdésekre magyarul válaszolj!\n",
    "Használd az alábbi dokumentumrészleteket forrásként a felhasználó kérdésének megválaszolásához!\n",
    "Ha azokból nem tudsz megadni releváns választ, akkor válaszold azt, hogy \"Sajnos erre nem tudok válaszolni, kérdezz mást a BME VIK szakmai gyakorlattal kapcsolatban\"\n",
    "\n",
    "DOKUMENTUMRÉSZLETEK:\n",
    "{context}\n",
    "\n",
    "ÚJ INPUT: {question}\n",
    "\n",
    "Válaszolj a kérdésre!\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(qa_template)\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=qa_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "combine_docs_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "chain = ConversationalRetrievalChain(\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    "    retriever = vectordb.as_retriever(\n",
    "        searh_type = search_type,\n",
    "        search_kwargs = {\n",
    "            \"k\": search_k,\n",
    "            \"fetch_k\": search_fetch_k,\n",
    "            \"lambda_mult\": lambda_mult\n",
    "        }\n",
    "    ),\n",
    "    question_generator=question_generator_chain,\n",
    "    memory=memory,\n",
    "    #return_generated_question=True,\n",
    "    rephrase_question=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio UI a chatbothoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio==3.47.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(message, history) -> str:\n",
    "    return chain({\"question\": message})[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interfész létrehozása, indítása:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_ui = gr.ChatInterface(qa, title = \"Lacibot\", description=\"Kérdezz a VIK-es szakmai gyakorlatról!\", undo_btn=None)\n",
    "chat_ui.launch() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesztelés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egyedüli kérdés\n",
    "question = \"Hogy hívják a tárgyfelelőst?\"\n",
    "chain({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==2.1.1 --quiet\n",
    "%pip install matplotlib==3.8.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Teszt kérdések betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_questions = pd.read_csv(\"testing/test_questions.csv\")\n",
    "print(len(test_questions))\n",
    "#test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vectordb(vectordb, embedding_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(columns=['Question', 'Document', 'Score'])\n",
    "\n",
    "    for question in test_questions['Question']:\n",
    "        print(question)\n",
    "        search_result = vectordb.similarity_search_with_relevance_scores(question, k = search_k)\n",
    "        for res in search_result:\n",
    "            df.loc[len(df)] = [question, res[0], res[1]]\n",
    "    \n",
    "    # elmentés\n",
    "    df.to_csv(f\"testing/embedding/{embedding_name}_k{search_k}_size{chunk_size}.csv\", index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Különböző embeddingekkel való tesztelés\n",
    "\n",
    "Használatához a fájl elején lévő adatok beolvasására van szükség."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install InstructorEmbedding --quiet\n",
    "%pip install sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "sentran_embedding = HuggingFaceEmbeddings(model_name= \"sentence-transformers/distiluse-base-multilingual-cased-v2\",model_kwargs={\"device\": \"cpu\"})\n",
    "sentran_vectordb = Chroma.from_documents(\n",
    "    documents=combined_data,\n",
    "    embedding=sentran_embedding,\n",
    "    persist_directory=persist_directory + \"sentran/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instr_embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-base\",\n",
    "    model_kwargs = {\"device\" : \"cpu\"},\n",
    "    query_instruction=\"Represent the query for retrieval: \"\n",
    ")\n",
    "instr_vectordb = Chroma.from_documents(\n",
    "    documents=combined_data,\n",
    "    embedding=instr_embeddings,\n",
    "    persist_directory=persist_directory + \"instr/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentran_df = test_vectordb(sentran_vectordb, \"sentence_tran\")\n",
    "instr_df = test_vectordb(instr_vectordb, \"instructor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_df = test_vectordb(oai_vectordb, \"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfok vectordb teszthez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_diagram(df, diagram_name: str):\n",
    "    grouped = df.groupby('Question')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Iterate over each group\n",
    "    for i, (name, group) in enumerate(grouped, start=1):\n",
    "        group = group.reset_index(drop=True)\n",
    "        ax.plot(group.index+1, group['Score'], label=f\"Group {i}\")\n",
    "\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    plt.xlabel('Index')\n",
    "    plt.xlim(0.75, search_k+0.25)\n",
    "    plt.xticks(range(1, search_k+1))\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(min(df['Score']) - 0.005, max(df['Score']) + 0.005)\n",
    "\n",
    "    plt.title(diagram_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_diagram(sentran_df, \"Sentence Transformer\")\n",
    "make_diagram(instr_df, \"Instructor\")\n",
    "make_diagram(openai_df, \"OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full tesztek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.2, model=model_id)\n",
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Kérdések egyenként"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Question', 'Human Answer', 'AI Answer', 'AI Evaluation'])\n",
    "for i, question in enumerate(test_questions['Question']):\n",
    "    memory.clear() # reset memory, hogy ne legyen hatása a következő kérdésre\n",
    "    ai_ans = chain({\"question\": question, \"chat_history\": memory.buffer})\n",
    "    eval_result = eval_chain({\"query\": question, \"result\": ai_ans['answer'], \"answer\": test_questions['Answer'][i]})\n",
    "    df.loc[len(df)] = [question, test_questions['Answer'][i], ai_ans['answer'], eval_result['results']]\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmentés\n",
    "df.to_csv(f\"testing/models/openai_k{search_k}_size{chunk_size}_newPrompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Összefüggő beszélgetés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chat = pd.read_csv(\"testing/test_chat.csv\")\n",
    "print(len(test_chat))\n",
    "#test_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "chain.verbose=True\n",
    "df = pd.DataFrame(columns=['Question', 'Human Answer', 'AI Answer', 'AI Evaluation'])\n",
    "for i, question in enumerate(test_chat['Question']):\n",
    "    ai_ans = chain({\"question\": question, \"chat_history\": memory.buffer})\n",
    "    eval_result = eval_chain({\"query\": question, \"result\": ai_ans['answer'], \"answer\": test_chat['Answer'][i]})\n",
    "    df.loc[len(df)] = [question, test_chat['Answer'][i], ai_ans['answer'], eval_result['results']]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmentés\n",
    "df.to_csv(f\"testing/models/openai_CHAT_k{search_k}_size{chunk_size}_newPrompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rouge-score==0.1.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(df, rouge_types: List[str]):\n",
    "    scores = []\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)\n",
    "    for index, row in df.iterrows():\n",
    "        #Mindegyik sorra az rouge score-t kiszámoljuk, majd a listába tesszük\n",
    "        score = scorer.score(row[\"Human Answer\"], row[\"AI Answer\"])\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eredmények betöltése\n",
    "df = pd.read_csv(\"testing/models/openai_k5_size500.csv\")\n",
    "chat_df = pd.read_csv(\"testing/models/openai_CHAT_k5_size500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge-2 és rouge-L score kiszámolása, minden sorra\n",
    "rouge2_scores = calculate_rouge(df, [\"rouge2\"])\n",
    "rougeL_scores = calculate_rouge(df, [\"rougeL\"])\n",
    "\n",
    "df = df.assign(ROUGE2=rouge2_scores, ROUGEL=rougeL_scores)\n",
    "df.to_csv(\"testing/models/openai_k5_size500.csv\", index=False)\n",
    "\n",
    "rouge2_chat_scores = calculate_rouge(chat_df, [\"rouge2\"])\n",
    "rougeL_chat_scores = calculate_rouge(chat_df, [\"rougeL\"])\n",
    "\n",
    "chat_df = chat_df.assign(ROUGE2=rouge2_chat_scores, ROUGEL=rougeL_chat_scores)\n",
    "chat_df.to_csv(\"testing/models/openai_CHAT_k5_size500.csv\", index=False)\n",
    "\n",
    "all_rouge2_scores = rouge2_scores + rouge2_chat_scores\n",
    "all_rougeL_scores = rougeL_scores + rougeL_chat_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# egy-egy sorozatból átlagszámítás\n",
    "def calculate_average(scores, rouge_type: str, version: str = \"\"):\n",
    "    precisions = [score[rouge_type][0] for score in scores]\n",
    "    p_avg = sum(precisions) / len(precisions)\n",
    "\n",
    "    recalls = [score[rouge_type][1] for score in scores]\n",
    "    r_avg = sum(recalls) / len(recalls)\n",
    "    \n",
    "    fmeasures = [score[rouge_type][2] for score in scores]\n",
    "    f_avg = sum(fmeasures) / len(fmeasures)\n",
    "\n",
    "    directory = \"testing/rouge/\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    file_name = f\"{directory}average_OpenAI{version}_{rouge_type}.txt\"\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(f\"Recall: {r_avg}\\n\")\n",
    "        f.write(f\"Precision: {p_avg}\\n\")\n",
    "        f.write(f\"F-measure: {f_avg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average(all_rouge2_scores, \"rouge2\")\n",
    "calculate_average(all_rougeL_scores, \"rougeL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
