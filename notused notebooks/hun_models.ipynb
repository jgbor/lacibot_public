{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7de1539",
   "metadata": {},
   "source": [
    "### Ezt az egészet átraktam Colabba, mert ott Pro előfizetéssel volt elég erőforrásom, hogy futtassam.\n",
    "\n",
    "bitsandbytes-szal sikerült ott kvantálni\n",
    "\n",
    "https://colab.research.google.com/drive/1AWF-efJb7woy2Jp1g3QRb0VmER19pQz_?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T10:16:56.191790300Z",
     "start_time": "2023-09-22T10:16:50.625528200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install transformers --quiet\n",
    "%pip install auto-gptq --quiet\n",
    "%pip install optimum --quiet\n",
    "%pip install accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043118921f62890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T10:17:28.626490800Z",
     "start_time": "2023-09-22T10:17:23.282746100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer,  AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from optimum.gptq import GPTQQuantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea345e",
   "metadata": {},
   "source": [
    "PULI GPTrio, online tesztkörnyezetben egész jó eredményeket adott\n",
    "![](docs/pics/puli_gptrio_test.png)\n",
    "![](docs/pics/puli_gptrio_test2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f804d55c25970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T10:42:12.061714600Z",
     "start_time": "2023-09-22T10:36:02.643123800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_id = \"NYTK/PULI-GPTrio\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f6254",
   "metadata": {},
   "source": [
    "Kvantálás GTPQ config alapján\n",
    "\n",
    "https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization\n",
    "\n",
    "Sok időt és erőforrást vesz ez is igénybe, többször kékhalált kapott a laptopom vagy csak teljesen kifagyott az egész semmire se reagálva.\n",
    "\n",
    "![](./docs/pics/kékhalál.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "quantizer = GPTQQuantizer(bits=4, dataset=\"c4\", block_name_to_quantize = \"model.decoder.layers\", model_seqlen = 2048)\n",
    "quantized_model = quantizer.quantize_model(model, tokenizer)\n",
    "\n",
    "save_folder = \"./quantized_models/PULI-GPTrio\"\n",
    "quantizer.save(model,save_folder)\n",
    "\n",
    "prompt = \"Elmesélek egy történetet a nyelvtechnológiáról.\"\n",
    "generator = pipeline(task=\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
    "\n",
    "print(generator(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48621f2a",
   "metadata": {},
   "source": [
    "Kvantálás PyTorch-csal\n",
    "\n",
    "\n",
    "![](./docs/pics/torch_q_fail1.png)\n",
    "\n",
    "Ennyi memóriának kéne lennie (16 gigából nem jön ki?????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as tq\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "quantized_model = tq.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "torch.save(quantized_model.state_dict(), './quantized_models/torch_q/quantized_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6155bc",
   "metadata": {},
   "source": [
    "bitsandbytes-sal\n",
    "\n",
    "Ram:\n",
    "![](./docs/pics/model8bit_ram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8efb1",
   "metadata": {},
   "source": [
    "Kvantálás nélkül"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8adae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "prompt = \"Elmesélek egy történetet a nyelvtechnológiáról.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48866e4",
   "metadata": {},
   "source": [
    "Nem tudom futtatni, Google Colabon és lokálisan is Out of memory errort kapok\n",
    "![](docs/pics/colab_run_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437643cf",
   "metadata": {},
   "source": [
    "huBERT fine tuned modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98859385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"mcsabai/huBert-fine-tuned-hungarian-squadv2\",\n",
    "    tokenizer=\"mcsabai/huBert-fine-tuned-hungarian-squadv2\",\n",
    "    top_k = 1,\n",
    "    handle_impossible_answer = True\n",
    ")\n",
    "\n",
    "prompt = {\n",
    "    'context': \"A Python egy általános célú, nagyon magas szintű programozási nyelv, melyet Guido van Rossum holland programozó kezdett el fejleszteni 1989 végén, majd hozott nyilvánosságra 1991-ben.[11] A nyelv tervezési filozófiája az olvashatóságot és a programozói munka megkönnyítését helyezi előtérbe a futási sebességgel szemben.[36][37][38] Például a behúzások szintaktikailag is fontosak. A Python többek között a funkcionális, az objektumorientált, az aspektusorientált az imperatív és a procedurális programozási paradigmákat támogatja. Dinamikus típusokat és automatikus memóriakezelést használ, ilyen szempontból hasonlít a Scheme, Perl és Ruby nyelvekhez, emellett szigorú típusrendszerrel rendelkezik. Erőssége a gazdag szabványos programkönyvtár.[40][41] A Python úgynevezett interpreteres nyelv, ami azt jelenti, hogy nincs különválasztva a forrás- és tárgykód, a megírt program máris futtatható, ha rendelkezünk a Python értelmezővel. A Python értelmezőt számos géptípusra és operációs rendszerre elkészítették, továbbá számtalan kiegészítő könyvtár készült hozzá, így rendkívül széles körben használhatóvá vált. Az egyik legnépszerűbb programozási nyelv.[42][43][44][45] Nyitott, közösségalapú fejlesztési modellt mutat fel, amit a közhasznú Python Software Foundation felügyel, ami a nyelv definícióját a CPython referenciaimplementációval gondozza. \",\n",
    "    'question': \"Ki találta ki a Pythont?\"\n",
    "}\n",
    "\n",
    "predictions = qa_pipeline(prompt)\n",
    "\n",
    "print(f'Kérdés: {prompt[\"question\"]}')\n",
    "print(f'Válasz: {predictions[\"answer\"]}')\n",
    "print(f'Konfidencia: {predictions[\"score\"]}')\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
