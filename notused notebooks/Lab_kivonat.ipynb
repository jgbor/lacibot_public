{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:40:59.142010700Z",
     "start_time": "2023-09-22T12:37:43.402268700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "    torchdata --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    rouge_score \\\n",
    "    loralib \\\n",
    "    peft --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:42:19.768548100Z",
     "start_time": "2023-09-22T12:42:11.739359200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:43:14.779935Z",
     "start_time": "2023-09-22T12:43:05.817547700Z"
    }
   },
   "outputs": [],
   "source": [
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:43:54.914018100Z",
     "start_time": "2023-09-22T12:43:30.929251700Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, ignore_mismatched_sizes=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:43:59.908992500Z",
     "start_time": "2023-09-22T12:43:59.867972500Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params=0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params +=param.numel()\n",
    "        if(param.requires_grad):\n",
    "            trainable_model_params +=param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall modell parameters: {all_model_params}\\npercentage of trainable model params: {100*trainable_model_params/all_model_params}\\n\"\n",
    "\n",
    "print (print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:44:06.013141100Z",
     "start_time": "2023-09-22T12:44:03.515940400Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = \"-\".join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL_GENERATION - ZERO SHOT:\\n{output}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Perform Full Fine-Tuning\n",
    "\n",
    "2.1 Preprocess the Dialog-Summary Dataset\n",
    "\n",
    "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with Summarize the following conversation and to the start of the summary with Summary as follows:\n",
    "\n",
    "Training prompt (dialogue):\n",
    "Summarize the following conversation.\n",
    "Chris: This is his part of the conversation.\n",
    "Antje: This is her part of the conversation.\n",
    "Summary:\n",
    "\n",
    "Training response (summary):\n",
    "Both Chris and Antje participated in the conversation.\n",
    "\n",
    "Then preprocess the prompt-response dataset into tokens and pull out their input_ids (1 per token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:53:10.040006400Z",
     "start_time": "2023-09-22T12:53:09.874731900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function (example):\n",
    "    start_prompt = 'Summarize the following conversation. \\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer (prompt, padding=\"max_length\", truncation =True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    example['labels'] = tokenizer (example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids \n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:53:24.084929900Z",
     "start_time": "2023-09-22T12:53:12.698432300Z"
    }
   },
   "outputs": [],
   "source": [
    "#subsample the dataset to save time\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 ==0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T10:43:55.112955300Z",
     "start_time": "2023-09-09T10:43:55.096667100Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#Check the shapes\n",
    "\n",
    "print(\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "pprint(tokenized_datasets['train'][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataset is ready for fine-tuning.\n",
    "\n",
    "2.2 - Fine-tune the model with the preprocessed dataset.\n",
    "Now utilize the built-in Hugging Face Trainer class. Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T10:45:28.712414Z",
     "start_time": "2023-09-09T10:45:28.672416800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=tokenized_datasets['train'], \n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T10:50:05.027853300Z",
     "start_time": "2023-09-09T10:45:42.313895500Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... skipping\n",
    "\n",
    "\n",
    "3.1 Setup PEFT/LoRA model for fine-tuning\n",
    "A nrew layer/adapter\n",
    "rank is the hyper parameter that defines the rank/dimension of the adapter to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T11:06:02.201107300Z",
     "start_time": "2023-09-09T11:06:02.180980100Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, #rank\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #FLAN-T5\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add lora adapter layers/parameters to the original LLM to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T11:06:19.526980900Z",
     "start_time": "2023-09-09T11:06:18.756901700Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model,\n",
    "                            lora_config)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = 0 if torch.cuda.is_available() else torch.device(\"cpu\") #Ugyanaz mint a fenti\n",
    "peft_model.to(device)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Train PEFT adapter\n",
    "Define training arguments and create Trainer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T11:07:20.693502700Z",
     "start_time": "2023-09-09T11:07:20.633776300Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T11:11:40.291440400Z",
     "start_time": "2023-09-09T11:07:44.442101600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A fenti nekem CPUn 127 perc volt a hatterben\n",
    "#####################################################3\n",
    "Prepare this model by adding an adapter to the original FlanT5 model. \n",
    "It will not be trainable because the plan is only to perform inference with this PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_trainer.save_model(\"./trained-for-long-time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T11:42:08.710496800Z",
     "start_time": "2023-09-09T11:41:56.956509300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from peft import PeftModel\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model_2 = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       './peft-dialogue-summary-checkpoint-local',\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model_2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Evaluate the model\n",
    "Make inferences for the same example as in sections 1.3 and 2.3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T12:00:28.820881300Z",
     "start_time": "2023-09-09T12:00:24.254701700Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 203\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'PEFT SUMMARY:\\n{peft_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
