{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbotysgxbhY3",
        "outputId": "8a8186a6-48e2-43ff-fbfe-992ea76439d1"
      },
      "outputs": [],
      "source": [
        "%pip install transformers==4.35.0 --quiet\n",
        "%pip install xformer==1.0.1 --quiet\n",
        "%pip install sentence-transformers==2.2.2 --quiet\n",
        "%pip install openai==0.28.1 --quiet \n",
        "%pip install langchain==0.0.320 --quiet\n",
        "%pip install chromadb==0.4.14 --quiet\n",
        "%pip install tiktoken==0.5.1--quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFjha4h5bhY4"
      },
      "source": [
        "### Paraméterek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25LlaBfobhY7"
      },
      "outputs": [],
      "source": [
        "# használt erőforrások mappája\n",
        "res_folder = \"res/in_use/\"\n",
        "    \n",
        "# splitting paraméterek\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50\n",
        "\n",
        "# vector store paraméterek\n",
        "persist_directory = \"res/chroma/\"\n",
        "search_type = \"mmr\"\n",
        "search_k = 5\n",
        "search_fetch_k = 8\n",
        "lambda_mult = 0.6\n",
        "\n",
        "# memória\n",
        "memory_k = 3\n",
        "\n",
        "# ChatGPT paraméterek\n",
        "temperature = 0.4\n",
        "max_tokens = 500\n",
        "model_id = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFWbkrKobhY8"
      },
      "source": [
        "## Erőforrásfájlok betöltése\n",
        "\n",
        "Ha már egyszer megtettük és van mentett vektor adatbázis, akkor nem kell újra futtatni.\n",
        "### CSV fájlok (nagyrészt kérdések) betöltése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkXTkuO9bhY9",
        "outputId": "1fbd051f-c79c-4e56-ee2a-a73099fdbab8"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import CSVLoader, DirectoryLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(res_folder, glob=\"*.csv\", use_multithreading=True, loader_cls=CSVLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
        "csv_data = directory_loader.load()\n",
        "print(len(csv_data))\n",
        "#csv_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76PSvbI1bhY-"
      },
      "source": [
        "### Szöveges fájlok betöltése, majd feldarabolása"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCbRYsK2bhY_",
        "outputId": "5fbc26de-a596-423e-fe73-1ea22849d572"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(res_folder, glob=\"*.txt\", use_multithreading=True, loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
        "text_data = directory_loader.load()\n",
        "len(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdLYeKNibhZA",
        "outputId": "dcbd1275-eb6b-4a6e-e5c4-df4fd3bd4167"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    length_function=len,\n",
        "    is_separator_regex=True,\n",
        "    separators=[\"\\n\\s*\\n\", \"\\n\\s*\", \"\\n\"]\n",
        ")\n",
        "\n",
        "split_text_data = text_splitter.split_documents(text_data)\n",
        "print(len(split_text_data))\n",
        "#split_text_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsCUXrcRbhZB"
      },
      "source": [
        "### VectorStore inicializálása Chroma-val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI API Key beállítása\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZPzvLWxbhZC"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "# Embedding betöltése\n",
        "instr_embeddings = HuggingFaceInstructEmbeddings( model_name=\"hkunlp/instructor-base\", model_kwargs = {\"device\" : \"cpu\"}, query_instruction=\"Represent the query for retrieval: \")\n",
        "embedding = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMriKIPxbhZD"
      },
      "outputs": [],
      "source": [
        "# Elég egyszer futtatni, ha nem változtatunk az adatokon, mert lementi a vektoradatbázist.\n",
        "# Változás esetén törölni kell a res/chroma maappát, majd újra kell futtatni.\n",
        "combined_data = []\n",
        "combined_data.extend(split_text_data)\n",
        "combined_data.extend(csv_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instructoros adatbázis\n",
        "instr_vectordb = Chroma.from_documents(\n",
        "    documents=combined_data,\n",
        "    embedding=instr_embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#OpeanAI adatbázis\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=combined_data,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlkrT7LbhZD"
      },
      "source": [
        "Ha le van már mentve, ezt kell használni az előző cella helyett:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZESSM7ObhZE"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcwY_bMCbhZE",
        "outputId": "ca4048b3-db0c-46d4-9ea7-5033e483f3f6"
      },
      "outputs": [],
      "source": [
        "# MMR teszt\n",
        "question = \"Ki a tárgyfelelős?\"\n",
        "search_result = vectordb.max_marginal_relevance_search(question,k = search_k, fetch_k = search_fetch_k, lambda_mult = lambda_mult)\n",
        "print(search_result)\n",
        "print(vectordb.similarity_search_with_relevance_scores(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLk-zHvjbhZF"
      },
      "source": [
        "### Memória config a chat historyhoz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXYrJgtGbhZF"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k = memory_k, memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok_xZ91wbhZF"
      },
      "source": [
        "## Chatbot\n",
        "\n",
        "### Saját prompttal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Előzmények alappján konkrét, önálló kérdést generálunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name= model_id, temperature = temperature, max_tokens = max_tokens)\n",
        "\n",
        "template = \"\"\"A chat előzményekből és egy következő kérdésből álló input alapján alakítsd át a következő kérdést akkor, ha a kérdés teljes értelmezéséhez szükséges a korábbi kontextus is, úgy, hogy az új kérdés értelmezhető legyen magában is.\n",
        "Nem fogalmmazz új kérdést bele a korábbi kontextus alapján, csak alakítsd át a kérdést, ha szükséges. Ha nem kapcsolódik szakmai gyakorlathoz a beszélgetés, akkor írd be, hogy \"Erre sajnos nem tudsz válaszolni\".\n",
        "Chat előzmények:\n",
        "    {chat_history}\n",
        "\n",
        "Eredeti kérdés: {question}\n",
        "Új kérdés:\"\"\"\n",
        "question_generator_prompt = PromptTemplate.from_template(template)\n",
        "question_generator_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=question_generator_prompt,\n",
        "    #verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "huBERT pipeline létrehozása és tesztelése:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "28634cf99af6461696c52e978e455ff7",
            "2d030d9513d74056859840ba76d20a7a",
            "38021aac8c9c407f9375b8898eab0719",
            "da4249dc43df4673b8c368891a41105e",
            "c64aaaed9ca64898b9179419f1271047",
            "cc1142c670fc44d395d43d6283b060c4",
            "7e9b77b7dd8a454ba9a7374a320e9a16",
            "80ee10b7311a49e18ec1b9143f12e625",
            "ee009badc93740adbf45381332159329",
            "3fb1aae3f404436db4823a407aeece8c",
            "4fc87b1053604defbf939d7e8ba46294"
          ]
        },
        "id": "FdKLcsofbhZG",
        "outputId": "be8feb58-add0-4191-e432-ba1971b264f7"
      },
      "outputs": [],
      "source": [
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"mcsabai/huBert-fine-tuned-hungarian-squadv2\"\n",
        "pipe = pipeline(task=\"question-answering\", model=model_name, tokenizer=model_name, top_k=1, handle_impossible_answer=False)\n",
        "qa_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "pipe({\"question\": \"Ki a tárgyfelelős?\", \"context\": \"A tárgyfelelős Blázovics László.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kérdést megválaszoló függvény:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ILas3eFVOSb"
      },
      "outputs": [],
      "source": [
        "memory.clear()\n",
        "\n",
        "def qa_answer(question) -> str:\n",
        "    if memory.buffer_as_str:\n",
        "        question = question_generator_chain({\"question\": question, \"chat_history\": memory.buffer})[\"text\"]\n",
        "        print(f'Új kérdés: {question}')\n",
        "    docs = instr_vectordb.max_marginal_relevance_search(question,k = search_k, fetch_k = search_fetch_k, lambda_mult = lambda_mult)\n",
        "    print(f'Keresési eredmények: {docs}')\n",
        "    all_content = \"\"\n",
        "    for doc in docs:\n",
        "        all_content += doc.page_content + \"\\n\\n\"\n",
        "    answer = pipe({\"question\": question, \"context\": all_content})[\"answer\"]\n",
        "    print(f'Válasz: {answer}')\n",
        "    if answer == \"\":\n",
        "        answer = \"Nem tudom a választ erre.\"\n",
        "    memory.save_context({\"input\": question}, {\"output\": answer})\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J50BkENDbhZK"
      },
      "source": [
        "## Gradio UI a chatbothoz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9DkEIaRbhZL",
        "outputId": "86e3e453-0f35-4146-caa2-73d0903df32c"
      },
      "outputs": [],
      "source": [
        "%pip install gradio==3.47.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qdtNm6IbhZM"
      },
      "outputs": [],
      "source": [
        "def qa(message, history) -> str:\n",
        "    return qa_answer(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jGargYnubhZN",
        "outputId": "9c43288d-7ada-4eb6-e737-bcf3ddc386a9"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "chat_ui = gr.ChatInterface(qa, title = \"Lacibot\", description=\"Kérdezz a VIK-es szakmai gyakorlatról!\", undo_btn=None)\n",
        "chat_ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvFk381ZbhZH"
      },
      "source": [
        "## Tesztelés\n",
        "\n",
        "#### Teszt kérdések betöltése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pandas==2.1.1 --quiet\n",
        "%pip install matplotlib==3.8.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3vSFYPkbhZH",
        "outputId": "89da0366-918b-48cc-f26c-d5bed6f8a944"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_questions = pd.read_csv(\"testing/test_questions.csv\")\n",
        "print(len(test_questions))\n",
        "#test_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructor vectoradatbázis tesztelése:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8n9fl_hbhZH",
        "outputId": "fd4f1523-4c80-444b-d100-89f0a7d70188"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(columns=['Question', 'Document', 'Score'])\n",
        "\n",
        "for question in test_questions:\n",
        "    search_result = instr_vectordb.similarity_search_with_relevance_scores(question, k = search_k)\n",
        "    for res in search_result:\n",
        "        df.loc[len(df)] = [question, res[0], res[1]]\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFP9rvM9bhZI"
      },
      "outputs": [],
      "source": [
        "# elmentés\n",
        "df.to_csv(f\"testing/results_k{search_k}_size{chunk_size}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSuWNyAZbhZI"
      },
      "source": [
        "#### Gráf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEaBggLibhZI",
        "outputId": "44b15736-50da-4b2a-8fe2-ff6665f2ba98"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grouped = df.groupby('Question')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Iterate over each group\n",
        "for i, (name, group) in enumerate(grouped, start=1):\n",
        "    group = group.reset_index(drop=True)\n",
        "    ax.plot(group.index+1, group['Score'], label=f\"Group {i}\")\n",
        "\n",
        "ax.legend(fontsize=8, loc='upper right')\n",
        "plt.xlabel('Index')\n",
        "plt.xlim(0.75, search_k+0.25)\n",
        "plt.xticks(range(1, search_k+1))\n",
        "\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(min(df['Score']) - 0.005, max(df['Score']) + 0.005)\n",
        "\n",
        "plt.title('Score for Each Question')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full tesztek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain.evaluation.qa import QAEvalChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=model_id)\n",
        "eval_chain = QAEvalChain.from_llm(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Kérdések egyenként"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['Question', 'Human Answer', 'AI Answer', 'AI Evaluation'])\n",
        "for i, question in enumerate(test_questions['Question']):\n",
        "    memory.clear() # reset memory, hogy ne legyen hatása a következő kérdésre\n",
        "    ai_ans = qa_answer(question)\n",
        "    eval_result = eval_chain({\"query\": question, \"result\": ai_ans, \"answer\": test_questions['Answer'][i]})\n",
        "    df.loc[len(df)] = [question, test_questions['Answer'][i], ai_ans, eval_result['results']]\n",
        "    \n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# elmentés\n",
        "df.to_csv(f\"testing/models/hubert_INSTRUCTOR_k{search_k}_size{chunk_size}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Összefüggő beszélgetés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_chat = pd.read_csv(\"testing/test_chat.csv\")\n",
        "print(len(test_chat))\n",
        "#test_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.clear()\n",
        "df = pd.DataFrame(columns=['Question', 'Human Answer', 'AI Answer', 'AI Evaluation'])\n",
        "for i, question in enumerate(test_chat['Question']):\n",
        "    ai_ans = qa_answer(question)\n",
        "    eval_result = eval_chain({\"query\": question, \"result\": ai_ans, \"answer\": test_chat['Answer'][i]})\n",
        "    df.loc[len(df)] = [question, test_chat['Answer'][i], ai_ans, eval_result['results']]\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# elmentés\n",
        "df.to_csv(f\"testing/models/hubert_INSTRUCTOR_CHAT_k{search_k}_size{chunk_size}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rouge score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install rouge-score==0.1.2 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge(df, rouge_types: List[str]):\n",
        "    scores = []\n",
        "    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)\n",
        "    for index, row in df.iterrows():\n",
        "        #Mindegyik sorra az rouge score-t kiszámoljuk, majd a listába tesszük\n",
        "        score = scorer.score(row[\"Human Answer\"], row[\"AI Answer\"])\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"testing/models/hubert_k5_size500.csv\")\n",
        "chat_df = pd.read_csv(\"testing/models/hubert_CHAT_k5_size500.csv\")\n",
        "\n",
        "instr_df = pd.read_csv(\"testing/models/hubert_INSTRUCTOR_k5_size500.csv\")\n",
        "instr_chat_df = pd.read_csv(\"testing/models/hubert_INSTRUCTOR_CHAT_k5_size500.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rouge2_scores = calculate_rouge(df, [\"rouge2\"])\n",
        "rougeL_scores = calculate_rouge(df, [\"rougeL\"])\n",
        "df = df.assign(ROUGE2=rouge2_scores, ROUGEL=rougeL_scores)\n",
        "df.to_csv(\"testing/models/hubert_k5_size500.csv\", index=False)\n",
        "\n",
        "rouge2_chat_scores = calculate_rouge(chat_df, [\"rouge2\"])\n",
        "rougeL_chat_scores = calculate_rouge(chat_df, [\"rougeL\"])\n",
        "chat_df = chat_df.assign(ROUGE2=rouge2_chat_scores, ROUGEL=rougeL_chat_scores)\n",
        "chat_df.to_csv(\"testing/models/hubert_CHAT_k5_size500.csv\", index=False)\n",
        "\n",
        "all_rouge2_scores = rouge2_scores + rouge2_chat_scores\n",
        "all_rougeL_scores = rougeL_scores + rougeL_chat_scores\n",
        "\n",
        "rouge2_instr_scores = calculate_rouge(instr_df, [\"rouge2\"])\n",
        "rougeL_instr_scores = calculate_rouge(instr_df, [\"rougeL\"])\n",
        "instr_df = instr_df.assign(ROUGE2=rouge2_instr_scores, ROUGEL=rougeL_instr_scores)\n",
        "instr_df.to_csv(\"testing/models/hubert_INSTRUCTOR_k5_size500.csv\", index=False)\n",
        "\n",
        "rouge2_instr_chat_scores = calculate_rouge(instr_chat_df, [\"rouge2\"])\n",
        "rougeL_instr_chat_scores = calculate_rouge(instr_chat_df, [\"rougeL\"])\n",
        "instr_chat_df = instr_chat_df.assign(ROUGE2=rouge2_instr_chat_scores, ROUGEL=rougeL_instr_chat_scores)\n",
        "instr_chat_df.to_csv(\"testing/models/hubert_INSTRUCTOR_CHAT_k5_size500.csv\", index=False)\n",
        "\n",
        "all_rouge2_instr_scores = rouge2_instr_scores + rouge2_instr_chat_scores\n",
        "all_rougeL_instr_scores = rougeL_instr_scores + rougeL_instr_chat_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def calculate_average(scores, rouge_type: str, version: str = \"\"):\n",
        "    precisions = [score[rouge_type][0] for score in scores]\n",
        "    p_avg = sum(precisions) / len(precisions)\n",
        "\n",
        "    recalls = [score[rouge_type][1] for score in scores]\n",
        "    r_avg = sum(recalls) / len(recalls)\n",
        "    \n",
        "    fmeasures = [score[rouge_type][2] for score in scores]\n",
        "    f_avg = sum(fmeasures) / len(fmeasures)\n",
        "\n",
        "    directory = \"testing/rouge/\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    file_name = f\"{directory}average_hubert{version}_{rouge_type}.txt\"\n",
        "\n",
        "    with open(file_name, 'w') as f:\n",
        "        f.write(f\"Recall: {r_avg}\\n\")\n",
        "        f.write(f\"Precision: {p_avg}\\n\")\n",
        "        f.write(f\"F-measure: {f_avg}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "calculate_average(all_rouge2_scores, \"rouge2\")\n",
        "calculate_average(all_rougeL_scores, \"rougeL\")\n",
        "\n",
        "calculate_average(all_rouge2_instr_scores, \"rouge2\", \"_INSTRUCTOR\")\n",
        "calculate_average(all_rougeL_instr_scores, \"rougeL\", \"_INSTRUCTOR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28634cf99af6461696c52e978e455ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d030d9513d74056859840ba76d20a7a",
              "IPY_MODEL_38021aac8c9c407f9375b8898eab0719",
              "IPY_MODEL_da4249dc43df4673b8c368891a41105e"
            ],
            "layout": "IPY_MODEL_c64aaaed9ca64898b9179419f1271047"
          }
        },
        "2d030d9513d74056859840ba76d20a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc1142c670fc44d395d43d6283b060c4",
            "placeholder": "​",
            "style": "IPY_MODEL_7e9b77b7dd8a454ba9a7374a320e9a16",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "38021aac8c9c407f9375b8898eab0719": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80ee10b7311a49e18ec1b9143f12e625",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee009badc93740adbf45381332159329",
            "value": 2
          }
        },
        "3fb1aae3f404436db4823a407aeece8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc87b1053604defbf939d7e8ba46294": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e9b77b7dd8a454ba9a7374a320e9a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80ee10b7311a49e18ec1b9143f12e625": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64aaaed9ca64898b9179419f1271047": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1142c670fc44d395d43d6283b060c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da4249dc43df4673b8c368891a41105e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb1aae3f404436db4823a407aeece8c",
            "placeholder": "​",
            "style": "IPY_MODEL_4fc87b1053604defbf939d7e8ba46294",
            "value": " 2/2 [01:17&lt;00:00, 36.92s/it]"
          }
        },
        "ee009badc93740adbf45381332159329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
