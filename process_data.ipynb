{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4 --quiet\n",
    "%pip install pandas --quiet\n",
    "%pip install lxml --quiet\n",
    "%pip install PyPDF2 --quiet\n",
    "%pip install openai==0.28.1 --quiet \n",
    "%pip install langchain==0.0.320 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Egyetemi szabályzat betöltése a pdf formátumból"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = 'https://www.vik.bme.hu/document/412/original/Szakmai_gyak_BSc_szabályzat_2014 után.pdf'\n",
    "response = requests.get(pdf_url)\n",
    "pdf_content = response.content\n",
    "\n",
    "if response.status_code == 200:\n",
    "    pdf_bytes = response.content\n",
    "    pdf_file = BytesIO(pdf_bytes)\n",
    "\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Az oldalak szövegének kinyerése és összefűzése\n",
    "    text = \"\"\n",
    "    for page in range(num_pages):\n",
    "        page_obj = pdf_reader.pages[page]\n",
    "        text += page_obj.extract_text()\n",
    "\n",
    "\n",
    "    with open('res/3.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    pdf_file.close()\n",
    "else:\n",
    "    print(\"Failed to download the PDF file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUT-os GYIK betöltése dataframe-be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.aut.bme.hu/SzakmaiGyakorlat/GyIK'\n",
    "qna = []\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "questions = soup.find_all(class_='question')\n",
    "for q in questions:\n",
    "    qna.append([q.get_text().strip(), q.find_next_sibling().get_text().strip()])\n",
    "\n",
    "qna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe kimentése csv fájlba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listából dataframe-et készítünk\n",
    "df = pd.DataFrame(qna, columns=['Question', 'Answer'])\n",
    "\n",
    "# dataframe-et csv-be mentjük\n",
    "df.to_csv('res/gyik.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 gyakorlatinfókkal teli oldal tartalmának kinyerése, ebből az egyik egy táblázat, ami dataframe-be kerül és úgy mentve az 1.txt-be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.aut.bme.hu/SzakmaiGyakorlat/', 'https://www.aut.bme.hu/SzakmaiGyakorlat/Teendok', 'https://www.vik.bme.hu/kepzes/gyakorlat/442.html']\n",
    "\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    #az oldalakon itt van a fő tartalom eltárolva\n",
    "    def has_role_main_or_id_main(tag):\n",
    "        return (tag.get('role') == 'main') or (tag.get('id') == 'main')\n",
    "\n",
    "    content = soup.find_all(has_role_main_or_id_main)\n",
    "\n",
    "    # A Teendok oldal egy táblázatot fog tartalmazni, ezt külön kell kezelni, akár külön cella is lehetne annak a kezelése\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    \n",
    "    if (urls.index(url) != 1):\n",
    "        with open(f'res/{urls.index(url)}.txt', 'w', encoding='utf-8') as f:\n",
    "            for elem in content:\n",
    "                text = elem.get_text()\n",
    "                # Sorokra bontjuk a szöveget, eltávolítjuk a felesleges whitespace-eket, és kihagyjuk az üres sorokat\n",
    "                lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "                # Ezek eután újra összraj a sorokat\n",
    "                text = '\\n'.join(lines)\n",
    "                f.write(text)\n",
    "    else:\n",
    "        # Az egyes indexű oldal egy táblázatot fog tartalmazni, ezt csv-be mentjük\n",
    "        for table in enumerate(tables):\n",
    "            todo_df = pd.read_html(str(table))[0]\n",
    "\n",
    "            todo_df.to_csv(f'./res/{urls.index(url)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kérdések generálása a többi oldalhoz is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource-ok betöltése, nem kell újra az oldalakat scrapelni\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(\"res\", glob=\"*/*.txt\", use_multithreading=True, loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "text_data = directory_loader.load()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# A szövegeket 250 karakteres chunk-okra bontjuk, amiknek 50 karakteres az átfedése\n",
    "splitter = TokenTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "\n",
    "splitted_data = splitter.split_documents(text_data)\n",
    "print(len(splitted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egy szövegrészlethez kérdések generálása\n",
    "def generate_faq_questions_answers(text):    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a FAQ question and answer generator.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "             Generate 15 FAQ-like question-answer pairs for the given text. If you don't know the answer, don't make up an answer and  don't give me the question .\n",
    "             Please provide your answer in Hungarian. Enter the question and answer separated by a ; character.\n",
    "             \n",
    "             Text:\n",
    "             {text}\n",
    "             \n",
    "             Example:\n",
    "             \"Mi a feladata a hallgatónak?\";\"A szakmai gyakorlati helyen a szakmai gyakorlati feladatokat elvégezze, és azt igazolja, illetve beszámolót készítsen hozzá.\"\n",
    "             \"Mikor kell felvenni a szakmai gyakorlat tárgyat?\";\"A szakmai gyakorlat félévében, nyári gyakorlat esetén a következő őszi félévben.\"\n",
    "             \"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.75,\n",
    "        #max_tokens=500,\n",
    "       )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = pd.DataFrame(columns=['Question', 'Answer'])\n",
    "\n",
    "for part in splitted_data:\n",
    "    prompt = part.page_content\n",
    "    response = generate_faq_questions_answers(prompt)\n",
    "    lines = response['choices'][0]['message']['content'].split('\\n')\n",
    "\n",
    "    data = []\n",
    "    # A válaszokat ; karakter mentén szétválasztjuk, és a kérdést és a választ külön-külön eltároljuk\n",
    "    for line in lines:\n",
    "        qna = line.split(';')\n",
    "        question = qna[0].strip('\"')\n",
    "        answer = qna[1].strip('\"')\n",
    "        data.append([question, answer])\n",
    "\n",
    "    # A generált kérdéseket és válaszokat egy dataframe-be mentjük\n",
    "    generated_df = pd.concat([generated_df, pd.DataFrame(data, columns=['Question','Answer'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df.to_csv(f'res/generated_qna{splitted_data.index(part)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate with Langchain (only in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model= \"gpt-3.5-turbo\")\n",
    "gen_chain = QAGenerateChain.from_llm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = gen_chain.apply_and_parse([{\"doc\": t} for t in splitted_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('res/qna.json', 'w') as f:\n",
    "    json.dump(questions_and_answers, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv delimiter kompatibilitás miatti rész (Excel ; delimiterrel dolgozik, ezért a review után ezzel mentette le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_df = pd.read_csv('res/not_in_use/generated_qna_GPT3.5_reviewed_excel.csv', delimiter=';')\n",
    "gpt35_df.to_csv('res/in_use/generated_qna_GPT3.5_reviewed.csv', index=False)\n",
    "\n",
    "gpt4_df = pd.read_csv('res/not_in_use/generated_qna_GPT4_reviewed_excel.csv', delimiter=';')\n",
    "gpt4_df.to_csv('res/in_use/generated_qna_GPT4_reviewed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blázovics Lacitól kapott mailekből személyes infók kivágása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# a kapott emailek beolvasása\n",
    "email_df = pd.read_csv('res/secret/SzakGyakQnA_original.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milye szavakat tartalmazó sorokat kell kihagyni\n",
    "remove_words = ['From:', 'To:', 'Cc:', 'Sent:', 'Date:', 'mailto',\n",
    "                'Feladó:', 'Küldve:', 'Címzett:', 'Másolatot kap:', 'Dátum:', 'Címe:',\n",
    "                'Kedves', 'Tisztelt',\n",
    "                'Üdvözlettel', 'Üdv', 'Tisztelettel', 'Köszönettel', 'Köszönöm', 'Válaszát', 'A választ'\n",
    "                'Neptun kód', 'Neptun:', 'Vírusmentes', 'Iroda:', 'Cím:', 'Mobil:', 'Cégbíróság:', 'Cg', 'Adószám:', 'Bankszámlaszám:', 'Email:' ,'Web:', '@',\n",
    "                '<https://www.avast.', '<http://www.avg', '+36', '06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(columns=['Emails'])\n",
    "\n",
    "for index, row in email_df.iterrows():\n",
    "  lines = row[0].split('\\n')\n",
    "  # Minden sor ellenőrzése, hogy tartalmazza-e a remove_words-ben megadott szavakat vagy üres-e vagy 6 karakter hosszú-e (talán neptunt ki lehetne szűrni így és más, fontosat nem)\n",
    "  clean_lines = [line for line in lines if not any(word in line.strip() for word in remove_words) and len(line) != 6]\n",
    "  # clean_lines lista összefűzése egy stringgé, majd a clean_df-hez adása\n",
    "  clean_df = pd.concat([clean_df, pd.DataFrame([['\\n'.join(clean_lines)]], columns=['Emails'])], ignore_index=True)\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('res/secret/SzakGyakQnA_cleaned.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kérdések generálása Openai-jal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_email(text):   \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a FAQ question and answer generator.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "             Generate a few FAQ-like question-answer pairs for the given text. The text is an email conversation stripped from most of the personal info. The latest email comes first, then the previous one, and so on. The last email was originally the first.\n",
    "             Please provide your answer in Hungarian. Enter the question and answer separated by a ; character as in a csv file.\n",
    "             \n",
    "             Email conversation:\n",
    "             {text}\n",
    "             \n",
    "             Example:\n",
    "             \"Mi a feladata a hallgatónak?\";\"A szakmai gyakorlati helyen a szakmai gyakorlati feladatokat elvégezze, és azt igazolja, illetve beszámolót készítsen hozzá.\"\n",
    "             \"Mikor kell felvenni a szakmai gyakorlat tárgyat?\";\"A szakmai gyakorlat félévében, nyári gyakorlat esetén a következő őszi félévben.\"\n",
    "             \"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.75,\n",
    "        #max_tokens=500,\n",
    "       )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "email_gen_df = pd.DataFrame(columns=['Question', 'Answer'])\n",
    "\n",
    "for index, row in clean_df.iterrows():\n",
    "    prompt = row['Emails']\n",
    "    response = generate_questions_from_email(prompt)\n",
    "    lines = response['choices'][0]['message']['content'].split('\\n')\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            qna = line.split(';')\n",
    "            question = qna[0].strip('\"')\n",
    "            answer = qna[1].strip('\"')\n",
    "            data.append([question, answer])\n",
    "        except Exception as e:\n",
    "           print(f\"Error processing line: {line}\")\n",
    "           print(f\"Error message: {str(e)}\")\n",
    "\n",
    "    email_gen_df = pd.concat([email_gen_df, pd.DataFrame(data, columns=['Question','Answer'])], ignore_index=True)\n",
    "    time.sleep(1)\n",
    "email_gen_df.to_csv(f'res/generated_email_qna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_gen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reviewed csv tagolása rossz itt is Excel miatt\n",
    "gen_email_rev_df = pd.read_csv('res/in_use/generated_email_qna_reviewed.csv', delimiter=';')\n",
    "gen_email_rev_df.to_csv('res/in_use/generated_email_qna_reviewed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
